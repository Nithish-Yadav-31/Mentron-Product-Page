<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        html, body {
            width: 100%;
            height: 100%;
            overflow: hidden;
            background-color: white;
            font-family: Arial, sans-serif;
        }
        
        #mindmap {
            width: 100%;
            height: 100vh;
            display: block;
        }
    </style>
    <script src="https://cdn.jsdelivr.net/npm/d3@6"></script>
    <script src="https://cdn.jsdelivr.net/npm/markmap-view"></script>
    <script src="https://cdn.jsdelivr.net/npm/markmap-lib@0.14.3/dist/browser/index.min.js"></script>
</head>
<body>
    <svg id="mindmap"></svg>
    <script>
        window.onload = async () => {
            try {
                const markdown = `# Machine Learning

This mindmap provides a comprehensive overview of machine learning concepts, algorithms, and techniques. It covers various aspects, including supervised and unsupervised learning, model evaluation, and natural language processing.

## Machine Learning Fundamentals

This branch covers the fundamental concepts of machine learning, including definitions, types of models, and the bias-variance trade-off.

*   **Definition:** Learning a target function *f* that maps input variables *X* to output variable *y* with an error *e*: *y = f(X) + e*.
    *   Emphasizes the goal of learning a mapping from inputs to outputs.
*   **Types of Models:** Supervised and Unsupervised learning.
    *   **Supervised Learning:** Learning to predict outcomes *y* from data points *X* given labeled data.
        *   Regression: Outcome is continuous.
            *   Conditional Expectation: E[y|X=x]
        *   Classification: Outcome is a class.
            *   Conditional Probability: P(Y=y|X=x)
    *   **Unsupervised Learning:** Learning to find inherent structure or hidden patterns from unlabeled data *X*.
*   **Model Types:** Discriminative and Generative Models.
    *   **Discriminative Model:** Learns the decision boundary between classes, directly estimating P(y|x).
        *   Examples: Logistic Regression, Random Forests, SVM, Neural Networks, Decision Tree, kNN.
    *   **Generative Model:** Learns the input distribution, estimating P(x|y) to find likelihood P(y|x) using Bayes' rule.
        *   Examples: Hidden Markov Models, Naive Bayes, Gaussian Mixture Models, Gaussian Discriminant Analysis, LDA, Bayesian Networks.
*   **Bias-Variance Trade-off:** Balancing erroneous assumptions (bias) with the model's sensitivity to training data (variance).
    *   **Bias:** Erroneous assumptions made by the model to make the target function *f* easier to learn.
    *   **Variance:** How much the predictions of the model will vary when trained on different training data.
    *   **Underfitting:** Model is not able to capture the trend in the data (High bias).
    *   **Overfitting:** Model learns too much from the available data but does not generalize well to new data (High variance).
*   **Training vs. Validation Loss:**
    *   Training loss goes down over time, achieving low error values.
    *   Validation loss goes down until a turning point is found, and it starts going up again, indicating overfitting.

## Model Evaluation and Validation

This branch focuses on evaluating the performance of machine learning models and preventing data leakage.

*   **Classification Problems:**
    *   **Confusion Matrix:** A table that describes the performance of a classification model.
        *   Type I error: The null hypothesis H0 is rejected when it is true (False Positive).
        *   Type II error: The null hypothesis H0 is not rejected when it is false (False Negative).
    *   **Accuracy:** Ratio of correct predictions over total predictions.
    *   **Recall/Sensitivity/True Positive Rate:** Completeness of model, how often the classifier is correct out of total actual positives.
    *   **Precision:** Exactness of model, how often the classifier is correct out of total predicted positives.
    *   **F1-Score:** Harmonic mean of precision and recall.
    *   **False Positive Rate:** Fraction of negatives wrongly classified as positive.
    *   **False Negative Rate:** Fraction of positives wrongly classified as negative.
    *   **Specificity:** Fraction of negatives rightly classified as negative.
    *   **ROC-curve: Trade-off between TPR and FPR using classification thresholds.
        *   AUC: Area Under the ROC Curve measures how likely the model differentiates positives and negatives.
    *   Precision-Recall curve: Focuses on the correct prediction of the minority class, useful when data is imbalanced.
*   **Regression Problems:**
    *   **Mean Squared Error (MSE):** Average of the squares of the errors.
    *   **Root Mean Squared Error (RMSE):** Square root of the MSE.
    *   **Mean Absolute Error (MAE):** Average of the absolute errors.
    *   **R-squared (R2):** Proportion of variance in the dependent variable that can be predicted from the independent variables.
    *   **Adjusted R-squared:** Adjusted R2 considers the number of predictors in the model.
*   **Cross-Validation:** Evaluates a model's performance by dividing the dataset into k subsets, training on k-1 folds, and testing on the remaining fold.
    *   **K-Fold Cross-Validation:** Data is divided into k equally-sized folds.
    *   **Stratified K-Fold:** Maintains the proportion of classes in each fold.
    *   **Leave-One-Out Cross-Validation (LOOCV):** k equals the number of data points.
*   **Preventing Data Leakage:**
    *   **Proper Data Splitting:** Split data into training, validation, and test sets before preprocessing.
    *   **Transformation fit on Training Data Only:** Ensure transformations are fit only on the training data.
    *   **Feature Selection:** Ensure features used for training are available at the time of prediction.
    *   **Data Augmentation:** Apply augmentation techniques only to the training set.
*   **Unrepresentative Datasets:**
    *   **Unrepresentative Training Dataset:** Model does not need to be valid.
    *   **Unrepresentative Validation Dataset:** Validation data is scarce and not very representative of the training data.
*   **Handling Missing/Corrupted Data:**
    *   Remove Missing Data.
    *   Impute Missing Data.
    *   Use Algorithms That Handle Missing Data.
    *   Replace Corrupted Data.
    *   Data Augmentation.

## Linear and Non-Linear Algorithms

This branch covers the core concepts and techniques used in linear and non-linear machine learning algorithms.

*   **Linear Algorithms:**
    *   **General Linear Regression Model:** *y = β0 + β1x1 + ... + βkxk + ϵ*.
        *   Ordinary Least Squares: Find β that minimizes squared error.
        *   Assumptions: Linearity, independence, normal errors, equal variance.
        *   Regularization: Ridge (L2), Lasso (L1).
    *   **Logistic Regression:** Models the probability of a binary outcome.
        *   Log-Odds and Logistics.
        *   Linear Discriminant Analysis (LDA): For multiclass classification.
    *   **Data Preparation for Linear Algorithms:**
        *   Data Transformation.
        *   Feature Engineering.
        *   Handling Outliers.
        *   Rescaling.
    *   **Advantages:**
        *   Simplicity and Interpretability.
        *   Computational Efficiency.
        *   Works Well with Linearly Separable Data.
*   **Non-Linear Algorithms:**
    *   **Naive Bayes Classifier:** Based on Bayes Theorem, assuming features are independent.
        *   Gaussian Naive Bayes.
    *   **Support Vector Machines (SVM):** Uses a hyperplane to separate classes with the largest margin.
        *   Kernel trick.
        *   Hyperparameters: regularization parameter (C) and the kernel parameters.
    *   **K-Nearest Neighbors (KNN):** Classifies based on the majority class among its k nearest neighbors.
    *   **Classification and Regression Trees (CART):** Decision trees for both classification and regression.
        *   Gini Impurity, Cross Entropy.
        *   Ensemble Algorithms: Bagging, Boosting.

## Optimization and Statistical Foundations

This branch covers the optimization algorithms and statistical concepts that underlie machine learning models.

*   **General Optimization Steps:**
    *   Understand data.
    *   Define loss function.
    *   Define predictive model.
    *   Search for parameters that minimize the loss function.
*   **Gradient Descent:** Iteratively moves in the direction of steepest descent to minimize a cost function.
    *   Stochastic Gradient Descent.
    *   Mini-batch Gradient Descent.
    *   Effective Gradient Descent: optimal learning rate, scale features, initialize parameters wisely, utilize mini-batch processing, monitor convergence, experiment with various optimizers, apply regularization techniques, avoid local minima, visualize loss trends, and tune hyperparameters diligently.
*   **Cost Function:** Commonly used to know the performance of a model.
    *   Regression: Mean Squared Error (MSE), Mean Absolute Error (MAE), Huber Loss, Log-Cosh Loss.
    *   Classification: Binary Cross-Entropy Loss, Categorical Cross-Entropy Loss, Sparse Categorical Cross-Entropy Loss, Hinge Loss, Squared Hinge Loss.
*   **Convex & Non-convex:**
    *   Convex function has one minimum.
    *   Non-convex function may find local minima instead of the global minimum.
*   **Likelihood and Posterior:**
    *   P(θ|y) = (P(y|θ)P(θ)) / P(y)
    *   Maximum Likelihood Estimation (MLE): Estimates parameters by maximizing the likelihood of the observed data.
    *   Bayesian Estimation - Maximum a Posterior (MAP): Finds parameters that maximize the posterior distribution.
*   **Variance Inflation Factor (VIF):** Measures the severity if multicollinearity.

## Advanced Machine Learning Topics

This branch covers more advanced topics in machine learning such as ensemble methods, neural networks, and time series analysis.

*   **Ensemble Algorithms:**
    *   **Bagging:** Parallel training of multiple models on different subsets of data.
        *   Random Forest.
    *   **Boosting:** Sequentially training of multiple models, where each model tries to correct the errors of the previous ones.
        *   AdaBoost.
        *   Gradient Boosting.
        *   XGBoost.
*   **Neural Networks:** Mimic the structure and function of the human brain.
    *   Input Layer, Hidden Layers, Output Layer.
    *   Activation Functions: Sigmoid, ReLU, Tanh, Softmax.
    *   Forward Propagation, Backpropagation.
    *   Regularization: Stopping training when validation performance drops, Dropout, Embedding weight penalties into the objective function, Batch Normalization.
    *   Recurrent Neural Network(RNN): designed to process sequences of data such as timeseries data, voice, natural language, and other activities.
        *   LSTM: LSTM memorizethe information for the long period of time. The difference between RNN and LSTM are: RNN cell has only one tanh layer while LSTM cell has four layers: forget gate layer, store gate layer, new cell state layer, output layer, and previous cell state as shown in Figure below.
    *   Convolutional Neural Network (CNN): well-suited for image classification and object recognition tasks.
*   **Unsupervised Machine Learning**
    *   Clustering
        *   K-means clustering
        *   Hierarchical Clustering
        *   DBSCAN
    *   Dimensionality Reduction
        *   Principle Component Analysis (PCA)
    *   Association Rule Mining
    *   Graphical Modelling and Network Analysis
*   **Time Series:**
    *   Time Series Statistical Models: Xt = mt + st + Yt.
        *   iid noise, Gaussian Noise, Random walk, White Noise.
        *   Moving Average Smoother.
        *   Exponential Smoothing.
        *   Double Exponential Smoothing.
        *   Triple Exponential Smoothing.
        *   ARIMA.
        *   SARIMA.
        *   Prophet.
        *   Generalized Additive Model.
    *   Stationary Process: Properties such as mean, variance, auto-correlation are constant over time.
        *   Non-Stationary: Trend and Seasonality.
*   **Natural Language Processing (NLP):**
    *   Text Processing: Cleaning, normalization, and conversion of raw text.
        *   Libraries: nltk, spacy.
        *   Lowercasing, Removing Punctuations, Tokenization, Lemmatization, Stemming, Language Detection.
    *   Feature Extraction: Converting text into numerical features.
        *   Bag-of-words.
        *   tf-idf.
        *   n-gram.
    *   Word Embedding: Numerical representation of words allowing similar words to have similar vector representations.
        *   word2vec.
        *   GloVe.
        *   BERT.
    *   Sentiment Analysis: Extracting attitudes and emotions from text.
    *   Topic Modelling: Capture the underlying themes that appear in documents
        *   LatentDirichletAllocation (LDA).
        *   LatentSemanticAnalysis (LSA).
*   **Recommender System:**
    *   Collaborative Filtering: Recommends what similar users like.
    *   Content Filtering: Recommends similar items.`;
                const transformer = new markmap.Transformer();
                const {root} = transformer.transform(markdown);
                const mm = new markmap.Markmap(document.querySelector('#mindmap'), {
                    maxWidth: 300,
                    color: (node) => {
                        const level = node.depth;
                        return ['#2196f3', '#4caf50', '#ff9800', '#f44336'][level % 4];
                    },
                    paddingX: 16,
                    autoFit: true,
                    initialExpandLevel: 2,
                    duration: 500,
                });
                mm.setData(root);
                mm.fit();
                
                // Listen for resize events from parent window
                window.addEventListener('resize', () => {
                    if (mm) {
                        mm.fit(); // Adjust mindmap when window is resized
                    }
                });
                
                // Handle message events from parent window (for focus mode)
                window.addEventListener('message', (event) => {
                    if (event.data && event.data.type === 'resize') {
                        if (mm) {
                            setTimeout(() => mm.fit(), 100); // Slight delay to ensure DOM has updated
                        }
                    }
                });
            } catch (error) {
                console.error('Error rendering mindmap:', error);
                document.body.innerHTML = '<div style="color: red; padding: 20px;">Error rendering mindmap. Please check the console for details.</div>';
            }
        };
    </script>
</body>
</html>
